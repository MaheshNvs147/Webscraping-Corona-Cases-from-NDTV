# -*- coding: utf-8 -*-
"""Webscraping CoronaCases from NDTV site .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j_3PkqbO9biX7m5tIcGIjExh31AABz1b

Steps for Scraping Any Website

Use requests library to get html content of the website

Fetching and parsing the data using Beautifulsoup and maintain the data in some data structure such as Dict or List.

Analyse the HTML tags and their attributes, such as class, id, and other HTML tag attributes. Also, identify your HTML tags where your content is present.

Output the data in any file format such as CSV, XLSX, JSON, DATAFRAME etc.
"""

url = "https://www.ndtv.com/coronavirus/india-covid-19-tracker"

import requests  #import requests library

page = requests.get(url)

page

page.text

from bs4 import BeautifulSoup

dir(BeautifulSoup)

soup = BeautifulSoup(page.text)

soup

soup.title

soup.table

soup = BeautifulSoup(page.text,'html.parser')

soup.table

table = soup.find_all('table')

table

rows = table[0].find_all('tr')

rows

for i in rows[0]:
  print(i.text)

[i.text for i in rows[0]]  # To get Headings

[i.text.replace('\n','') for i in rows[0]]  #Replace \n with empty string

[i.text.replace('\n','').strip() for i in rows[0]]  #Remove empty spaces

headings = [i.text.replace('\n','').strip() for i in rows[0]]  # Assigning a variable

rows = table[0].find_all('tr')

rows

for i in rows:
  print(i)

all_rows = []
for i in rows:
  td = i.find_all('td')
  row = [i.text for i in td]
  all_rows.append(row)              #get the rows in a list

all_rows

all_rows.pop(0)   #remove first element

all_rows

for i in all_rows:
  i[0] = i[0][:i[0].find("DistrictCases")]   #Remove the part of first element after 'DistrictCases'

for i in all_rows:
  i[1] = i[1][:i[1].find(" ")]
  i[2] = i[2][:i[2].find(" ")]
  i[3] = i[3][:i[3].find(" ")]
  i[4] = i[4][:i[4].find(" ")]  #remove inc/dec part after space

all_rows #data after cleansing

import pandas as pd
df = pd.DataFrame(all_rows,columns=headings)

df

